%
% This is a borrowed LaTeX template file for lecture notes for CS267,
% Applications of Parallel Computing, UCBerkeley EECS Department.
% Now being used for CMU's 10725 Fall 2012 Optimization course
% taught by Geoff Gordon and Ryan Tibshirani.  When preparing 
% LaTeX notes for this class, please use this template.
%
% To familiarize yourself with this template, the body contains
% some examples of its use.  Look them over.  Then you can
% run LaTeX on this file.  After you have LaTeXed this file then
% you can look over the result either by printing it out with
% dvips or using xdvi. "pdflatex template.tex" should also work.
%

\documentclass[twoside]{article}
\setlength{\oddsidemargin}{0.25 in}
\setlength{\evensidemargin}{-0.25 in}
\setlength{\topmargin}{-0.6 in}
\setlength{\textwidth}{6.5 in}
\setlength{\textheight}{8.5 in}
\setlength{\headsep}{0.75 in}
\setlength{\parindent}{0 in}
\setlength{\parskip}{0.1 in}

%
% ADD PACKAGES here:
%

\usepackage{amsmath,amsfonts,graphicx}
\usepackage{listing}
\usepackage{mathrsfs}

\usepackage{color}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}


%
% The following commands set up the lecnum (lecture number)
% counter and make various numbering schemes work relative
% to the lecture number.
%
\newcounter{lecnum}
\renewcommand{\thepage}{\thelecnum-\arabic{page}}
\renewcommand{\thesection}{\thelecnum.\arabic{section}}
\renewcommand{\theequation}{\thelecnum.\arabic{equation}}
\renewcommand{\thefigure}{\thelecnum.\arabic{figure}}
\renewcommand{\thetable}{\thelecnum.\arabic{table}}

%
% The following macro is used to generate the header.
%
\newcommand{\lecture}[4]{
   \pagestyle{myheadings}
   \thispagestyle{plain}
   \newpage
   \setcounter{lecnum}{#1}
   \setcounter{page}{1}
   \noindent
   \begin{center}
   \framebox{
      \vbox{\vspace{2mm}
    \hbox to 6.28in { {\bf 01: Convex Optimization
	\hfill May 2016} }
       \vspace{4mm}
       \hbox to 6.28in { {\Large \hfill Lecture #1: #2  \hfill} }
       \vspace{2mm}
       \hbox to 6.28in { {\it Author: #3 \hfill Scribes: #4} }
      \vspace{2mm}}
   }
   \end{center}
   \markboth{Lecture #1: #2}{Lecture #1: #2}

   {\bf Note}: {\it Reading notes for complex optimization.}

   {\bf Disclaimer}: {\it These notes have not been subjected to the
   usual scrutiny reserved for formal publications.  They may be distributed
   outside this class only with the permission of Mr. Yan.}
   \vspace*{4mm}
}
%
% Convention for citations is authors' initials followed by the year.
% For example, to cite a paper by Leighton and Maggs you would type
% \cite{LM89}, and to cite a paper by Strassen you would type \cite{S69}.
% (To avoid bibliography problems, for now we redefine the \cite command.)
% Also commands that create a suitable format for the reference list.
\renewcommand{\cite}[1]{[#1]}
\def\beginrefs{\begin{list}%
        {[\arabic{equation}]}{\usecounter{equation}
         \setlength{\leftmargin}{2.0truecm}\setlength{\labelsep}{0.4truecm}%
         \setlength{\labelwidth}{1.6truecm}}}
\def\endrefs{\end{list}}
\def\bibentry#1{\item[\hbox{[#1]}]}

%Use this command for a figure; it puts a figure in wherever you want it.
%usage: \fig{NUMBER}{SPACE-IN-INCHES}{CAPTION}
\newcommand{\fig}[3]{
			\vspace{#2}
			\begin{center}
			Figure \thelecnum.#1:~#3
			\end{center}
	}
% Use these for theorems, lemmas, proofs, etc.
\newtheorem{theorem}{Theorem}[lecnum]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newenvironment{proof}{{\bf Proof:}}{\hfill\rule{2mm}{2mm}}

% **** IF YOU WANT TO DEFINE ADDITIONAL MACROS FOR YOURSELF, PUT THEM HERE:

\newcommand\E{\mathbb{E}}

\begin{document}
%FILL IN THE RIGHT INFO.
%\lecture{**LECTURE-NUMBER**}{**DATE**}{**LECTURER**}{**SCRIBE**}
\lecture{1}{Introduction to Complex Optimization}{Christopher M. Bishop}{scribe-name1,2,3}
%\footnotetext{These notes are partially based on those of Nigel Mansell.}

% **** YOUR NOTES GO HERE:

% Some general latex examples and examples making use of the
% macros follow.  
%**** IN GENERAL, BE BRIEF. LONG SCRIBE NOTES, NO MATTER HOW WELL WRITTEN,
%**** ARE NEVER READ BY ANYBODY.

%----------------------------------------------------------------------------------------
%	INTRO
%----------------------------------------------------------------------------------------
\section{Solving optimization problems}
\begin{enumerate}
\item general optimization problem
\begin{enumerate}
\item very difficult to solve
\item methods involve some compromise, e.g. very long computation time, or not always finding the solution
\end{enumerate}
\item  exception: certain problem classes can be solved efficiently and reliably
\begin{enumerate}
\item least squares problems
\item linear programming problems
\item convex optimization problems
\end{enumerate}
\end{enumerate}
%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Least-squares}

minimize 
\section{Maximum Margin Classifiers} 

Two-class classification problem:
\begin{equation}
y(x) = w^T \phi(x) + b
\end{equation}
where $\phi$ denotes a fixed feature-space transformation, $ b$ explicit.

Training set comprises N input vectors $x_1, \cdots, x_N$ with target $t_1, /cdots, t_N$ where $t_N \in \{-1, 1\}$ and new data points are classified according to the sign of $y(x)$.

Training data - linearly separable in feature space, so  that there exits one choice of $w$ and $b$ such that by 1.1 satisfies $y(x_n)>0$ for points having $t_n=+1$ and $y(x_n)<0$ for $t_n=-1$.

In section 4, the perceptron algorithm is guaranteed to find a solution for the linearly separable problem. The solution deeps on the initial value and the order of the data points are presented - if there are multiple solutions we should try to find one with the smallest generalizaiton error.
SVM approaches this problem through the concept of the \textbf{margin} - the smallest distance between the decision boundary and any of the samples.

Perpendicular distance of a point $x$ from a hyperplane: $ {\lvert y(x) \rvert} / \lVert w \rVert$. We are only interested in solutions for which all data points are correctly classified ($t_n y(x_n)) >0$ for all n). So we have the distance of a point $x_n$ to the decision surface is given by:
\begin{equation}
\frac{t_ny(x_n)}{\lVert w \rVert} = \frac{t_n(w^T \phi(x_n)+b)}{\lVert w \rVert}
\end{equation}

{\color{red}{The margin is given by the perpendicular distance to the closest point $x_n$ from the data set, $w$ and $b$ need optimized in order to maximize the distance}}.

Then the maximum margin solution is found by
\begin{equation}
\argmax_{w, b} \{ \frac{1}{\lVert w \rVert} \min_n[t_n(w^T \phi(x_n)+b)]\}
\end{equation}

in which $\lVert w \rVert$ does not depend on $n$.
The direct solution of this optimization problem would be complex $\Rightarrow$ an equivalent problem easier to solve:

\begin{equation}
w \rightarrow \kappa w
\end{equation}

\begin{equation}
b \rightarrow \kappa b
\end{equation}
the rescaling would not change the distance.
We then use this freedom to set 
\begin{equation}
t_n(w^T \phi(x_n)+b)=1
\end{equation}
for the point that is closest to the surface.  

The canonical representation of the decision hyperplane:
\begin{equation}
t_n(w^T \phi(x_n)+b) \geq 1,   n = 1,\cdots, N
\end{equation}
When the equality holds, the constraints are said to be active.

There would always be at least one active constraint, because there will always be a closest point, and once the margin has been maximized there will be at leat two active constraints.  The optimization problem minimizing $\lVert w \rVert ^2$ is equivalent to maximize $\lVert w \rVert^{-1}$, so we need to solve the problem:
\begin{equation}
\argmin_{w,b} \frac{1}{2} \lVert w \rVert ^2
\end{equation}
subject to the constraints given by 1.7.

We introduce the Lagrange multipliers $a_n \geq 0$ with one multiplier $a_n$ for each of the constraints in1.7 giving the Lagrangian function:

\begin{equation}
L(w,b,a) = \frac{1}{2} \lVert w \rVert^2 - \sum_{n=1}^{N} a_n \{t_n(w^T \phi(x_n)+b)-1\}
\end{equation}
where $a=(a_1,\cdots,a_N)^T$.

Setting the derivatives of $L(w, b, a)$ with respect to $w$ abd $b$ equal to zero, we obtain the following conditions:
\begin{equation}
w= \sum_{n=1}^{N} a_n t_n \phi(x_n)
\end{equation}

\begin{equation}
0 = \sum_{n=1}^{N}a_nt_n
\end{equation}
Eliminating $w$ and $b$ from $L(w, b, a)$ using these conditions then gives the dual representation of the \textbf{maximum margin problem} in which we maximize
\begin{equation}
\tilde{L}(a) = \sum_{n=1}^{N}a_n - \frac{1}{2}\sum_{n=1}^{N}\sum_{m=1}^{N} a_na_mt_nt_mk(x_n,x_m)
\end{equation}
with respect to a subject to the constraints
\begin{equation}
a_n \geq 0,n = 1,\cdots,N
\end{equation}
\begin{equation}
\sum_{n=1}^{N}  a_nt_n = 0
\end{equation}

Here the kernel function is defined by $k(x, x')=\phi(x)^T\phi(x')$. This takes the form of a \textbf{quadratic programming problem}.

Computational complexity - $O(M^3)$.

{\color{red}{With dual formulation, the original optimization problem involved minimizing over $M$ variables into the dual problem with $N$ variables}}.

For fixed set of basis functions $M$ is smaller than the data points number $N$, dual representation seems useless.
However, it allows the model to be reformulated using kernels, and the maximum margin classifier can be applied efficiently to feature spaces whose dimensionality $M$ exceeds the number of data points $N$, including infinite feature spaces.

The kernel formulation also makes clear the role of the constraint that the kernel function $k(x, x')$ be \textbf{positive definite} - this ensures the Lagrangian function $\tilde{J}(a)$ is bounded below, giving rise to a well-defined optimization problem.

So new data point can be evaluated by:
\begin{equation}
y(x) = w^T\phi(x)+b = \sum_{n=1}^{N} a_n t_n k(x, x_n)+b
\end{equation}

For the Karush-Kuhn-Tucker KKT condtion, which in this case require that the following three properties hold:
\begin{equation}
a_n\geq0
\end{equation}
\begin{equation}
t_ny(x_n) - 1 \geq 0
\end{equation}
\begin{equation}
a_n\{t_n y(x_n)-1\} = 0
\end{equation}

------------------------------

check the Appendix E

------------------------------

For every data point, either $a_n=0$ or $t_ny(x_n)=1$, any data point for which $a_n=0$ will not appear in the sum in1.15 thus play no role in making predictions for new data points. \textbf{The remaining data points are called support vectors} - and they satisfy $t_ny(x_n)=1$, they lie on the maximum margin hyperplanes in feature space -- once the model is trained only the support vectors retained.

Solving the quadratic programming problem for $a$, determine $b$ by 
\begin{equation}
t_n(\sum_{m\in \mathcal{S}} a_mt_mk(x_n, x_m)+b)=1
\end{equation} where $\mathcal{S}$ denotes the set of indices of the support vectors.

A more stable solution for b is:
\begin{equation}
b=\frac{1}{N_{\mathcal{S}}} \sum_{m\in \mathcal{S}} (t_n - \sum_{m\in \mathcal{S}} a_mt_mk(x_n, x_m) )
\end{equation}
where $N_{\mathcal{S}}$ is the total number of support vectors.

\subsection{Overlapping class distributions}

\subsection{Relation to logistic regression}

\subsection{Multiclass SVMs}

\subsection{SVMs for regression}

\subsection{Computational learning theory}


%----------------------------------------------------------------------------------------
%	INTRO
%----------------------------------------------------------------------------------------
\section{Relevance Vector Machines} 

\subsection{RVM for regression}

\subsection{Analysis of sparsity}

\subsection{RVM for classification}

%----------------------------------------------------------------------------------------
%	INTRO
%----------------------------------------------------------------------------------------





%----------------------------------------------------------------------------------------
%	INTRO
%----------------------------------------------------------------------------------------






%----------------------------------------------------------------------------------------
%	INTRO
%----------------------------------------------------------------------------------------






%----------------------------------------------------------------------------------------
%	INTRO
%----------------------------------------------------------------------------------------







%----------------------------------------------------------------------------------------
%	INTRO
%----------------------------------------------------------------------------------------

We now delve right into the proof.

\begin{lemma}
This is the first lemma of the lecture.
\end{lemma}

\begin{proof}
The proof is by induction on $\ldots$.
For fun, we throw in a figure.
%%%NOTE USAGE !
\fig{1}{1in}{A Fun Figure}

This is the end of the proof, which is marked with a little box.
\end{proof}

\subsection{A few items of note}

Here is an itemized list:
\begin{itemize}
\item this is the first item;
\item this is the second item.
\end{itemize}

Here is an enumerated list:
\begin{enumerate}
\item this is the first item;
\item this is the second item.
\end{enumerate}

Here is an exercise:

{\bf Exercise:}  Show that ${\rm P}\ne{\rm NP}$.

Here is how to define things in the proper mathematical style.
Let $f_k$ be the $AND-OR$ function, defined by

\[ f_k(x_1, x_2, \ldots, x_{2^k}) = \left\{ \begin{array}{ll}

	x_1 & \mbox{if $k = 0$;} \\

	AND(f_{k-1}(x_1, \ldots, x_{2^{k-1}}),
	   f_{k-1}(x_{2^{k-1} + 1}, \ldots, x_{2^k}))
	 & \mbox{if $k$ is even;} \\

	OR(f_{k-1}(x_1, \ldots, x_{2^{k-1}}),
	   f_{k-1}(x_{2^{k-1} + 1}, \ldots, x_{2^k}))	
	& \mbox{otherwise.} 
	\end{array}
	\right. \]

\begin{theorem}
This is the first theorem.
\end{theorem}

\begin{proof}
This is the proof of the first theorem. We show how to write pseudo-code now.
%*** USE PSEUDO-CODE ONLY IF IT IS CLEARER THAN AN ENGLISH DESCRIPTION

Consider a comparison between $x$ and~$y$:
\begin{tabbing}
\hspace*{.25in} \= \hspace*{.25in} \= \hspace*{.25in} \= \hspace*{.25in} \= \hspace*{.25in} \=\kill
\>{\bf if} $x$ or $y$ or both are in $S$ {\bf then } \\
\>\> answer accordingly \\
\>{\bf else} \\
\>\>    Make the element with the larger score (say $x$) win the comparison \\
\>\> {\bf if} $F(x) + F(y) < \frac{n}{t-1}$ {\bf then} \\%
\>\>\> $F(x) \leftarrow F(x) + F(y)$ \\
\>\>\> $F(y) \leftarrow 0$ \\
\>\> {\bf else}  \\
\>\>\> $S \leftarrow S \cup \{ x \} $ \\
\>\>\> $r \leftarrow r+1$ \\
\>\> {\bf endif} \\
\>{\bf endif} 
\end{tabbing}

This concludes the proof.
\end{proof}


\section{Next topic}

Here is a citation, just for fun~\cite{CW87}.

\section*{References}
\beginrefs
\bibentry{CW87}{\sc D.~Coppersmith} and {\sc S.~Winograd}, 
``Matrix multiplication via arithmetic progressions,''
{\it Proceedings of the 19th ACM Symposium on Theory of Computing},
1987, pp.~1--6.
\endrefs

% **** THIS ENDS THE EXAMPLES. DON'T DELETE THE FOLLOWING LINE:

\end{document}





